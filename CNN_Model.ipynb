{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBKpofIOod5Tu2Xt2J0wkz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CFeenan/SolarCNN/blob/master/CNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_6nu0wCEmpfW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN1D, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 7, 64)  # 61 → 30 → 15 → 7 (after 3 poolings)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(64, 1)  # Output is a single logit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))  # Now shape: (batch, 128, 7)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # No sigmoid here — handled in BCEWithLogitsLoss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load the .npy files for train and test data\n",
        "X_train = np.load(\"X_train.npy\")\n",
        "y_train = np.load(\"y_train.npy\")\n",
        "X_test = np.load(\"X_test.npy\")\n",
        "y_test = np.load(\"y_test.npy\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Output the shapes for confirmation\n",
        "X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OIV9smxnHh5",
        "outputId": "946b3c71-0ebf-4743-a591-06b33ed88c01"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3750, 3, 61]),\n",
              " torch.Size([3750]),\n",
              " torch.Size([750, 3, 61]),\n",
              " torch.Size([750]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "# Define model\n",
        "model = CNN1D()\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Compute class imbalance weight\n",
        "healthy, faulty = torch.bincount(y_train_tensor.long())\n",
        "pos_weight = torch.tensor([2.0], dtype=torch.float32).to(device)\n",
        "\n",
        "# Loss function with class weight\n",
        "criterion = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Optimiser\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwf_CNuWnTMg",
        "outputId": "295552ba-1dd4-4d28-8229-3d2257d6f96e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Avg Loss: 0.5198\n",
            "Epoch 2, Avg Loss: 0.3132\n",
            "Epoch 3, Avg Loss: 0.3074\n",
            "Epoch 4, Avg Loss: 0.2609\n",
            "Epoch 5, Avg Loss: 0.2248\n",
            "Epoch 6, Avg Loss: 0.2246\n",
            "Epoch 7, Avg Loss: 0.2098\n",
            "Epoch 8, Avg Loss: 0.2262\n",
            "Epoch 9, Avg Loss: 0.2063\n",
            "Epoch 10, Avg Loss: 0.1778\n",
            "Epoch 11, Avg Loss: 0.1778\n",
            "Epoch 12, Avg Loss: 0.1853\n",
            "Epoch 13, Avg Loss: 0.1667\n",
            "Epoch 14, Avg Loss: 0.1676\n",
            "Epoch 15, Avg Loss: 0.1484\n",
            "Epoch 16, Avg Loss: 0.2061\n",
            "Epoch 17, Avg Loss: 0.1635\n",
            "Epoch 18, Avg Loss: 0.2068\n",
            "Epoch 19, Avg Loss: 0.1712\n",
            "Epoch 20, Avg Loss: 0.1435\n",
            "Epoch 21, Avg Loss: 0.1709\n",
            "Epoch 22, Avg Loss: 0.1548\n",
            "Epoch 23, Avg Loss: 0.1348\n",
            "Epoch 24, Avg Loss: 0.1537\n",
            "Epoch 25, Avg Loss: 0.1157\n",
            "Epoch 26, Avg Loss: 0.1273\n",
            "Epoch 27, Avg Loss: 0.1206\n",
            "Epoch 28, Avg Loss: 0.1410\n",
            "Epoch 29, Avg Loss: 0.1317\n",
            "Epoch 30, Avg Loss: 0.1151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "predictions, actuals = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs).squeeze()\n",
        "        preds = (outputs > 0.45).int()\n",
        "        predictions.extend(preds.tolist())\n",
        "        actuals.extend(labels.int().tolist())\n",
        "\n",
        "print(confusion_matrix(actuals, predictions))\n",
        "print(classification_report(actuals, predictions, target_names=[\"Healthy\", \"Faulty\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y-xcQlDnXn2",
        "outputId": "bd1d7f77-f1b5-4d56-b033-e9aaa9ec3532"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[556  14]\n",
            " [ 41 139]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.93      0.98      0.95       570\n",
            "      Faulty       0.91      0.77      0.83       180\n",
            "\n",
            "    accuracy                           0.93       750\n",
            "   macro avg       0.92      0.87      0.89       750\n",
            "weighted avg       0.93      0.93      0.92       750\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"cnn_model_v4.pth\")\n"
      ],
      "metadata": {
        "id": "CHA0i9OWDPps"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}